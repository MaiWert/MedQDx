#  MedQDx: Diagnose Like a Doctor 

MedQDx is a cutting-edge benchmark that simulates realistic, partial clinical scenarios to evaluate large language models’ diagnostic strategies in reaching a diagnosis through adaptive, question-driven reasoning.

![image](https://github.com/user-attachments/assets/a4278d45-781e-4022-9c2f-cf50c0549947)

##  MedQDx Data
### Raw Data - [Symptom‑Disease Prediction Dataset (SDPD)](https://data.mendeley.com/datasets/dv5z3v2xyd/1).

The Symptom-Disease Prediction Dataset (SDPD) is a comprehensive collection of structured data linking symptoms to various diseases, meticulously curated to facilitate research and development in predictive healthcare analytics. This dataset provides a comprehensive collection of disease names and associated symptoms, encoded in a one-hot manner.
**Size:** 2564 rows | 400 symptoms | 133 unique diseases

Each row in the dataset represents a single instance, where it can be 0/1 for each disease:
* 1 indicates the symptom is relevant to the disease case, 0 implies no relevance.
* Contains patient records mapping sets of symptoms to diagnoses (prognoses).
* Original CSV file: `symbipredict_2022.csv`.




### Generated patient cases
Patient cases generated by MedLlama2 based on Diseases and their Symptoms dataset
| Column          | Column Description                      |
|----------------|------------------------------|
| `Disease`    | Disease name |
| `Full_case` | Patient case with all symptoms associated with the disease       |
| `80_Percent_Case`  | The patient case with approximately 80% of the symptoms      |
| `50_Percent_Case`   | The patient case with approximately 50% of the symptoms         |

##  Table of Contents

1. [💡 Introduction](#💡-introduction)
2. [🗂️ Project Structure](#🗂️-project-structure)
3. [⚙️ Getting Started](#⚙️-getting-started)
4. [📚 Submodule Overviews](#📚-submodule-overviews)

   * [🧪 EDA & Baseline](#🧪-eda--baseline)
   * [🎬 Benchmark Creation](#🎬-benchmark-creation)
   * [📊 Evaluation](#📊-evaluation)
5. [📈 Presentation](#📈-presentation)
6. [🤝 Contributing](#🤝-contributing)



## 💡 Introduction
Patients rarely present a complete clinical picture at first, so physicians must engage in dynamic, targeted questioning to uncover critical details. The ability to conduct an adaptive dialogue—asking the right follow-up questions at each step—is therefore essential for high-quality diagnostic reasoning and treatment planning.

Large language models (LLMs) have demonstrated impressive capabilities in medical natural language understanding and generation, and are increasingly being integrated as diagnostic support tools in clinical workflows. However, existing benchmarks typically evaluate LLMs on fully revealed patient cases, without measuring their capacity for conducting strategic inquiry under partial clinical picture.

**MedQDx** addresses this gap by simulating realistic diagnostic uncertainty through an interactive, multi-round question-and-answer format. In MedQDx, an LLM “doctor” must iteratively question an LLM “patient” using only partial case information (50% of the data) and make a diagnosis after each round. This benchmark enables robust evaluation of an LLM’s ability to adapt its questioning strategy, refine its hypotheses, and ultimately align its predictions with the ground-truth diagnosis.  

By requiring multi-turn doctor–patient interactions, we push AI from passive responders to active clinical reasoning partners.


*  **Zero-Shot Diagnostic Accuracy (ZDA)**
*  **Mean Question-based Diagnostic Similarity (MQD)**
*  *  **Mean of Max Similarity Across Row (MMS)**

## 🗂️ Project Structure

```bash
MedQDx/                                  
├── EDA and Baseline/                     
│   ├── 📄 EDA & Baseline README.md
│   ├──  MedQDx__EDA_and_Baseline.ipynb
│   └──  Patient cases.csv
├── Benchmark Creation/                   
│   ├── 📄 Benchmark Creation README.md
│   ├──  MedQDx_Benchmark_Creation.ipynb
│   └──  MedQDx_Benchmark.csv
├── Benchmark Evaluation/                          
│   ├── 📄 Evaluation README.md
│   └──  MedQDx_Evaluation.ipynb
├── Presentations/
│   ├──  MedQDx - Final Presentation.pdf
│   ├──  MedQDx - Interim Presentation.pdf
│   └──  MedQDx - Project proposal.pdf
└── 📘 README.md                           # ← You are here!
```


## ⚙️ Getting Started

1. **Clone the repository**

   ```bash
   git clone https://github.com/MaiWert/MedQDx.git
   cd MedQDx
   ```
2. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```
3. **Configure credentials**

   ```bash
   export MedQDx_ENDPOINT="https://<your-azure-endpoint>"
   export MedQDx_API_KEY="<your-api-key>"
   export MedQDx_API_VERSION="2024-12-01-preview"
   ```
4. **Launch notebooks**

   * 🧪 `EDA & Baseline/MedQDx__EDA_and_Baseline.ipynb`
   * 🎬 `Benchmark Creation/MedQDx_Benchmark_Creation.ipynb`
   * 📊 `Evaluation/MedQDx_Evaluation.ipynb`



## 📚 Submodule Overviews

### 🧪 EDA & Baseline

Dive into the Symptom–Disease Prediction Dataset (SDPD) with cleaning, exploratory analysis, and baseline patient case generation. Covers 100%, 80%, and 50% symptom reveals with Jaccard analysis.

> 🔗 [Explore](./EDA%20and%20Baseline/EDA%20%26%20Baseline%20README.md)

### 🎬 Benchmark Creation

Simulate doctor–patient dialogues using different LLM personas. Collect questions, answers, and diagnoses for each partial case and export detailed conversation logs.

> 🔗 [Simulate](./Benchmark%20Creation/Benchmark%20Creation%20README.md)

### 📊 Evaluation

Assess the AI diagnostician’s performance by calculating ZDA, similarity metrics, and visualizing results across symptom completeness tiers.

> 🔗 [Evaluate](./Evaluation/Evaluation%20README.md)


## 🤝 Contributing

Your ideas make MedQDx better! Follow these steps to contribute:

1. 🔀 Fork the repo
2. 🌱 Create a branch: `git checkout -b feature/your-awesome-idea`
3. 💾 Commit: `git commit -m "Add amazing feature"`
4. 🔄 Push & PR: `git push origin feature/your-awesome-idea`


---

*✨ MedQDx © 2025 Mai Werthaim & Maya Kimhi*
